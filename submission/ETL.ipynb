{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7df590",
   "metadata": {},
   "source": [
    "# <ins>\"_3M 2024 CBG Data Engineering Internship Take-Home Coding Assessment_\"</ins>\n",
    "\n",
    "- A lightweight simple ETL system to parse files, transform as per database expectations and ingest them.\n",
    "- ETL and SQLConnector class is made generic that can be re-used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "766e3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author: Meghana Hegde\n",
    "# ETL system\n",
    "# \n",
    "# 02/2024\n",
    "\n",
    "# bunch of libraries that will come handy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import sqlite3\n",
    "from   sqlite3 import Error\n",
    "from   collections import defaultdict \n",
    "import sql_queries\n",
    "\n",
    "# data directory of all files\n",
    "DATA_DIR = os.path.dirname(os.getcwd()) + '/data'\n",
    "\n",
    "# set this to False if cleaned/processed data need not be saved into files.\n",
    "PROCESSED_DATA_SAVE = True\n",
    "\n",
    "PROCESSED_DATA_DIR = os.getcwd() + '/PROCESSED'\n",
    "if not os.path.exists(PROCESSED_DATA_DIR) and PROCESSED_DATA_SAVE is True:\n",
    "    os.makedirs(PROCESSED_DATA_DIR)\n",
    "\n",
    "# set this to False if you need to reduce the output/ETL logging.\n",
    "DEBUG = True\n",
    "\n",
    "# set this to False if analysis is not required (prelim analysis dump etc)\n",
    "ANALYZE = True\n",
    "\n",
    "# pretty names for logging\n",
    "DF_MAP_KEY_TO_NAMES = {\n",
    "    'trans_csv' : 'Transactions Data (CSV)',\n",
    "    'trans_json': 'Transactions Data (JSON)',\n",
    "    'trans_txt' : 'Transactions Data  (TEXT)',\n",
    "    'weather'   : 'Weather Data',\n",
    "    'location'  : 'Location Data',\n",
    "    'holiday'   : 'Holiday Data',\n",
    "    'date'      : 'Date Data (Derived)'\n",
    "}\n",
    "\n",
    "# dataframe types to their attributes mapping\n",
    "# value = [[l1], [l2], [l3]]\n",
    "# first list is the list of columns on which duplicates/NA values need to be dropped before insertion\n",
    "# second list is the column mapper of the corresponding tables in sql db.\n",
    "# third list is the database table name it maps to.\n",
    "DF_MAP_KEY_TO_FIELDS = {\n",
    "    'trans'   : [['transaction_id', 'location_id', 'date', 'profit'], ['transaction_id', 'location_id', 'date', 'profit'], ['transactions']],\n",
    "    'location': [['location_id'], ['location_id', 'elevation', 'population'], ['location']],\n",
    "    'date'    : [[], ['date', 'day', 'day_of_week', 'month', 'year', 'holiday'], ['date']],\n",
    "    'weather' : [['location_id','date'], ['date', 'location_id', 'temperature', 'pressure', 'humidity', 'cloudy', 'precipitation'], ['weather']],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18308e52",
   "metadata": {},
   "source": [
    "# <ins>Preliminary Analysis of data</ins>\n",
    "- Weather data is missing for many days where transaction data is available.\n",
    "- CSV file data for transactions has profit data with special character \\$ in it which needs to be handled. Loss is indicated with brackets around the value which also needs to be handled by ETL.\n",
    "- TEXT file data for transactions has profit data with leading 0 (multiple 0's) in its value which will get handled in dataframe as float and leading zeroes will be removed.\n",
    "- TEXT file data for transactions has negative profit values as eg: 0-X.X - this also needs to be handled by our ETL with some regex pattern.\n",
    "- Transaction data files across 3 systems have different date formatting. eg M-D-YYYY, M/D/YYYY, YYYY-M-D which needs to be generalized before inserting into database by our ETL.\n",
    "- Many float values in weather data for pressure etc are not rounded to 2 decimals. ETL needs to handle this.\n",
    "\n",
    "Most of it is covered in ETL's _prelimAnalysis_ and _transformData_ method's.\n",
    "\n",
    "\n",
    "# <ins>Data Quality checks</ins>\n",
    "- Remove duplicate rows, if any. There are no duplicates in this dataset.\n",
    "- Missing data for some columns - NA/NULL values for transactions profit data.\n",
    "    - in the preliminary analysis we see that very small \\% of data indeed has N/A or NULL values. Less than ~30 rows for 80K+ rows (transactions). In this case I feel, it is ok to completely drop such rows as it doesnt lead to significant data loss.\n",
    "    - However, this will lead to some loss of information. We should also consider at doing data imputation by either mean/median imputation or forward/backward fill imputation (future work)\n",
    "    - 27 transaction records have been dropped due to NULL profit data.\n",
    "    - If profit is dependent on any other variable, then linear regression can also be explored for data imputation (future work)\n",
    "    \n",
    "- _Weather_ data has location id's _011, 012, 013_ which have no corresponding transaction data so they are useless for our analysis. There are 3995 such records. Hence these rows are not required in location table as well. However I have kept it.\n",
    "- There are 390 records in transactions data with missing weather data.\n",
    "- There are 0 records in transaction data which are major holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dfc70de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETL:\n",
    "    \"\"\" ETL class for extract, transform, ingest and some analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, connector):\n",
    "        \"\"\" Initialize object variables\n",
    "        \"\"\"\n",
    "        self.dataFrameMap = {}\n",
    "        self.sqlConn      = connector\n",
    "        self.stats        = {'writes': 0, 'reads': 0}\n",
    "        \n",
    "    def parseFilesToDf(self):\n",
    "        \"\"\" A parser method that walks over all files in a data directory\n",
    "            and parses files of interest and stores them in \n",
    "            dataframe dict of df's per type of data.\n",
    "        \"\"\"\n",
    "        trans_csv_dfs  = []\n",
    "        trans_json_dfs = []\n",
    "        trans_txt_dfs  = []\n",
    "        weather_dfs    = []\n",
    "        location_dfs   = []\n",
    "        holiday_dfs    = []\n",
    "        \n",
    "        # Walk over all files of interest.\n",
    "        try:\n",
    "            for file in os.listdir(DATA_DIR):\n",
    "                file_path = DATA_DIR + '/' + file\n",
    "                \n",
    "                # parse transactions files.\n",
    "                if file.startswith('transactions'):\n",
    "                    \n",
    "                    # get the extension of the file\n",
    "                    _, extension = os.path.splitext(file)\n",
    "                \n",
    "                    if extension == \".csv\":\n",
    "                        try:\n",
    "                            # transid, location id have leading 0's - parse as it is as string and not int.\n",
    "                            df_csv = pd.read_csv(file_path, dtype={'transaction_id': str, 'location_id': str})\n",
    "                        except Exception as ex:\n",
    "                            # we should probably log this so that file parsing errors are known\n",
    "                            continue\n",
    "                            \n",
    "                        # storing filename in df temporarily\n",
    "                        df_csv['file_name'] = file\n",
    "                        trans_csv_dfs.append(df_csv)\n",
    "                    \n",
    "                    elif extension == '.txt':\n",
    "                        try:\n",
    "                            df_txt = pd.read_csv(file_path, sep='\\t', dtype={'transaction_id': str, 'location_id': str})\n",
    "                        except Exception as ex:\n",
    "                            continue\n",
    "                        df_txt['file_name'] = file\n",
    "                        trans_txt_dfs.append(df_txt)\n",
    "                \n",
    "                    elif extension == '.json':\n",
    "                        try:\n",
    "                            df_json = pd.read_json(file_path, dtype={'transaction_id': str, 'location_id': str})\n",
    "                        except Exception as ex:\n",
    "                            continue\n",
    "                        df_json['file_name'] = file\n",
    "                        trans_json_dfs.append(df_json)\n",
    "                    else:\n",
    "                        print(f\"Unsupported file {file}\")\n",
    "                    \n",
    "                elif file.startswith('location'):\n",
    "                    # these will always be csv \n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, dtype={'location_id': str})\n",
    "                    except Exception as ex:\n",
    "                        continue\n",
    "                    df['file_name'] = file\n",
    "                    location_dfs.append(df)\n",
    "                \n",
    "                elif file.startswith('weather'):\n",
    "                    # these will always be csv\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, dtype={'location_id': str})\n",
    "                    except Exception as ex:\n",
    "                        continue\n",
    "                    df['file_name'] = file\n",
    "                    weather_dfs.append(df)\n",
    "                    \n",
    "                elif file.startswith('holiday'):\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                    except Exception as ex:\n",
    "                        continue\n",
    "                    holiday_dfs.append(df)\n",
    "                    \n",
    "            # concat all df's of similar type\n",
    "            self.dataFrameMap['trans_csv'] = pd.concat(trans_csv_dfs)\n",
    "            self.dataFrameMap['trans_txt'] = pd.concat(trans_txt_dfs)\n",
    "            self.dataFrameMap['trans_json'] = pd.concat(trans_json_dfs)\n",
    "            self.dataFrameMap['weather'] = pd.concat(weather_dfs)\n",
    "            self.dataFrameMap['location'] = pd.concat(location_dfs)\n",
    "            self.dataFrameMap['holiday']  = pd.concat(holiday_dfs)\n",
    "                    \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "    def prelimAnalysis(self):\n",
    "        \"\"\" Prints preliminary analysis of a data frame.\n",
    "        \"\"\"\n",
    "        merged_trans_df_list = []\n",
    "        count = 0\n",
    "        \n",
    "        for key, data_frame in self.dataFrameMap.items():\n",
    "            # text to print\n",
    "            text = DF_MAP_KEY_TO_NAMES[key]\n",
    "            print(f\"\\n\\nPreliminary Analysis of {text}\")\n",
    "            print(\"-----------------------------------------------\\n\")\n",
    "            \n",
    "            try:\n",
    "                if 'trans' in key:\n",
    "                    merged_trans_df_list.append(data_frame)\n",
    "                \n",
    "                # Top 5 rows to get overview of data\n",
    "                print(data_frame.head().to_markdown())\n",
    "\n",
    "                # check information about data whether data is null, how many rows are appended to dataframe\n",
    "                print(\"\\n=====> Information about data\")\n",
    "                data_frame.info()\n",
    "\n",
    "                # check how many rows there in original file? how many appended\n",
    "                print(f\"\\n=====> There are total {data_frame.shape[0]} rows in {text}\")\n",
    "\n",
    "                # Null check for any rows in dataframe\n",
    "                null_values = data_frame.columns[data_frame.isnull().any()]\n",
    "                \n",
    "                print(f\"\\n=====> There are {len(null_values)} columns which has NULL values\")\n",
    "                \n",
    "                # Check for duplicate rows\n",
    "                duplicates = data_frame[data_frame.duplicated()]\n",
    "\n",
    "                # Display the duplicate rows, if any\n",
    "                if not duplicates.empty:\n",
    "                    print(f\"\\n=====> Following are {len(duplicates)} duplicate Rows\")\n",
    "                    print(duplicates)\n",
    "                else:\n",
    "                    print(\"\\n=====> No duplicate rows found.\")\n",
    "                    \n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            merged_trans_df = pd.concat(merged_trans_df_list)\n",
    "            location_df = self.dataFrameMap['location']\n",
    "            weather_df = self.dataFrameMap['weather']\n",
    "            date_df = self.dataFrameMap['date']\n",
    "\n",
    "            loc_ids = set(location_df['location_id'])\n",
    "            trans_loc_ids = set(merged_trans_df['location_id'])\n",
    "\n",
    "            # this is to find all such locations which dont have transaction data\n",
    "            missing_ids = loc_ids - trans_loc_ids\n",
    "\n",
    "            for id in missing_ids:\n",
    "                print(f\"\\n\\n=====> Location ID {id} has no corresponding transactions data\")\n",
    "        \n",
    "            # try to find weather data for irrelevant locations (location that dont have trans data)\n",
    "            # these rows are of no use for our analysis.\n",
    "            weather_ids = set(weather_df['location_id'])\n",
    "            for id in [i for i in weather_df['location_id']]:\n",
    "                if id in list(missing_ids):\n",
    "                    count += 1\n",
    "                    \n",
    "            print(f\"\\n=====> There are {count} records in weather data for locations with no transactions data\")\n",
    "            \n",
    "            # this is to find all such data in transaction that dont have weather data for that day and loc\n",
    "            loc_ids = set(zip(weather_df['location_id'], pd.to_datetime(weather_df['date'], format='mixed')))\n",
    "            trans_loc_ids = set(zip(merged_trans_df['location_id'], pd.to_datetime(merged_trans_df['date'], format='mixed')))\n",
    "            \n",
    "            missing_ids = trans_loc_ids - loc_ids\n",
    "\n",
    "            print(f\"\\n=====> There are {len(missing_ids)} such records in transactions data with missing weather data\")    \n",
    "            \n",
    "            # find all dates which are holidays\n",
    "            date_ids = set(date_df[date_df['holiday'] == 1]['date'])\n",
    "            \n",
    "            # extract uniques dates from the date column\n",
    "            trans_date_ids = set(merged_trans_df['date'])\n",
    "            \n",
    "            # find the overlap\n",
    "            common_dates = date_ids.intersection(trans_date_ids)\n",
    "            \n",
    "            print(f\"\\n=====> There are {len(common_dates)} records in transaction data for dates which are major holidays\")\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "                \n",
    "    def createDateDataFrame(self):\n",
    "        \"\"\" Create a date dateframe to load all values from January 2019 to October 2022 as mentioned in project\n",
    "            We need to derive date, day, day_of_week, month and year\n",
    "            To derive holiday column we need to left join with existing date dataframe with \n",
    "            holiday_data dataframe to fetch holiday column\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create a date range from Jan 2019 to Oct 2022 (you can change end date and make it current date)\n",
    "            date_start_end = pd.date_range(start='2019-01-01', end='2022-10-31', freq='D').strftime('%m/%d/%Y')\n",
    "\n",
    "            # Create a DataFrame with the date range\n",
    "            date_df = pd.DataFrame({'date': date_start_end})\n",
    "\n",
    "            date_df['date'] = pd.to_datetime(date_df['date'])\n",
    "\n",
    "            # Add a new column 'Day' to represent the day of the week\n",
    "            date_df['day'] = date_df['date'].dt.day\n",
    "            date_df['day_of_week'] = date_df['date'].dt.dayofweek\n",
    "            date_df['month'] = date_df['date'].dt.month\n",
    "            date_df['year'] = date_df['date'].dt.year\n",
    "\n",
    "            # Fetch Holidays data\n",
    "            holiday_df = self.dataFrameMap['holiday']\n",
    "            holiday_df['date'] = pd.to_datetime(holiday_df['date'])\n",
    "\n",
    "            # Left join of date with holidays_data on date to fetch holiday column\n",
    "            date_df = date_df.merge(holiday_df, how='left', on='date')\n",
    "\n",
    "            # If Holiday then True else False\n",
    "            date_df['holiday'] = date_df['holiday'].notna()\n",
    "\n",
    "            # update the dataFrameMap with these values\n",
    "            self.dataFrameMap['date'] = date_df\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "        \n",
    "    def transformData(self):\n",
    "        \"\"\" Does some data transformations based on dataframe type and fields to suit database needs\n",
    "            - Deals with duplicate rows, rows with N/A or NULL values\n",
    "            - Generalize data format of different columns as per database expectations.\n",
    "            - data cleaning for profit values in transactions etc\n",
    "            - cleaning other fields\n",
    "        \"\"\"\n",
    "        for key, data_frame in self.dataFrameMap.items():\n",
    "            if key in ['holiday']:\n",
    "                # these wont go into DB\n",
    "                continue\n",
    "                \n",
    "            # pretty text for print's\n",
    "            text = DF_MAP_KEY_TO_NAMES[key]\n",
    "            \n",
    "            # this is the key to index the KEY TO FIELDS dict\n",
    "            subset_key = key.split(\"_\")[0]\n",
    "\n",
    "            # columns in DF\n",
    "            df_columns = data_frame.columns.values.tolist()\n",
    "            \n",
    "            # columns in DB\n",
    "            db_columns = DF_MAP_KEY_TO_FIELDS[subset_key][1]\n",
    "            \n",
    "            # Retrieve rows before dropping\n",
    "            nof_rows_before = data_frame.shape[0]\n",
    "            if DEBUG:\n",
    "                print(f\"Total number of records before transformation = {nof_rows_before} for {text}\")\n",
    "\n",
    "            # Remove rows with null data based on a subset of columns\n",
    "            if len(DF_MAP_KEY_TO_FIELDS[subset_key][0]) > 0:\n",
    "                # consider a subset of columns - if they have null values drop it \n",
    "                data_frame = data_frame.dropna(subset=DF_MAP_KEY_TO_FIELDS[subset_key][0])\n",
    "                nof_rows_after = data_frame.shape[0]\n",
    "                if DEBUG:\n",
    "                    print(f\"No of null rows dropped = {nof_rows_before - nof_rows_after}\")\n",
    "\n",
    "            # Remove duplicates\n",
    "            nof_rows_before = data_frame.shape[0]\n",
    "            if len(DF_MAP_KEY_TO_FIELDS[subset_key][0]) > 0:\n",
    "                data_frame = data_frame.drop_duplicates(subset=DF_MAP_KEY_TO_FIELDS[subset_key][0])\n",
    "                nof_rows_after = data_frame.shape[0]\n",
    "                if DEBUG:\n",
    "                    print(f\"No of duplicate rows dropped = {nof_rows_before - nof_rows_after}\")\n",
    "                \n",
    "            # some transformations as per db needs\n",
    "            if 'date' in df_columns:\n",
    "                # standardize different formats that we have into one format\n",
    "                data_frame['date'] = pd.to_datetime(data_frame['date']).dt.strftime('%Y-%m-%d')\n",
    "            if 'temperature' in df_columns:\n",
    "                # keep upto 2 decimals\n",
    "                data_frame['temperature'] = data_frame['temperature'].round(2)\n",
    "            if 'pressure' in df_columns:\n",
    "                # keep upto 2 decimals\n",
    "                data_frame['pressure'] = data_frame['pressure'].round(2)\n",
    "            if 'profit' in df_columns:\n",
    "                # strip leading $ character, handle negative profit values in different format etc.\n",
    "                if data_frame['profit'].dtype == 'O':\n",
    "                    data_frame['profit'] = data_frame['profit'].str.replace('[$, 0]', '', regex=True)\n",
    "                    data_frame['profit'] = pd.to_numeric(data_frame['profit'].str.replace('[(]', '-', regex=True), errors='coerce')\n",
    "            \n",
    "            # Arrange column names as per db schema\n",
    "            self.dataFrameMap[key] = data_frame[db_columns]\n",
    "            \n",
    "            if DEBUG:\n",
    "                nof_rows_after = self.dataFrameMap[key].shape[0]\n",
    "                print(f\"Total number of records after transformation = {nof_rows_after} for {text}\")\n",
    "                print(f\"Summary of {text}\")\n",
    "                print(self.dataFrameMap[key])\n",
    "                self.dataFrameMap[key].info()\n",
    "                \n",
    "    def saveProcessedData(self):\n",
    "        \"\"\" Optionally save the processed data in cleaned files\n",
    "            Every iteration the files are truncated first and written\n",
    "        \"\"\"\n",
    "        \n",
    "        merged_df_list = []\n",
    "        \n",
    "        if PROCESSED_DATA_SAVE is False:\n",
    "            return\n",
    "        \n",
    "        for key in ['trans_json', 'trans_csv', 'trans_txt']:\n",
    "            # gather all transactions df's from 3 systems for transactions\n",
    "            merged_df_list.append(self.dataFrameMap[key])\n",
    "            \n",
    "        merged_df = pd.concat(merged_df_list)\n",
    "    \n",
    "        # flush all the dataframes into files (dont write column names to file)\n",
    "        try:\n",
    "            merged_df.to_csv(PROCESSED_DATA_DIR + '/transactions.csv', header=False)\n",
    "            self.dataFrameMap['location'].to_csv(PROCESSED_DATA_DIR + '/location.csv', header=False)\n",
    "            self.dataFrameMap['weather'].to_csv(PROCESSED_DATA_DIR + '/weather.csv', header=False)\n",
    "            self.dataFrameMap['date'].to_csv(PROCESSED_DATA_DIR + '/date.csv', header=False)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "    def ingestData(self):\n",
    "        \"\"\" Ingests the data in database\n",
    "        \"\"\"\n",
    "        \n",
    "        for key, data_frame in self.dataFrameMap.items():\n",
    "            try:\n",
    "                if key in ['holiday']:\n",
    "                    # these wont go into DB\n",
    "                    continue\n",
    "\n",
    "                # this is the key to index the KEY TO FIELDS dict\n",
    "                subset_key = key.split(\"_\")[0]\n",
    "\n",
    "                # pretty text\n",
    "                text = DF_MAP_KEY_TO_NAMES[key]\n",
    "\n",
    "                # table name\n",
    "                table = DF_MAP_KEY_TO_FIELDS[subset_key][2][0]\n",
    "                \n",
    "                # flush the dataframe to db\n",
    "                nof_writes = self.sqlConn.executeWriteQueries(data_frame, table)\n",
    "                \n",
    "                self.stats['writes'] += nof_writes\n",
    "                print(f\"Wrote {nof_writes} rows to table {table}\")\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                \n",
    "    def readData(self, query, df_columns):\n",
    "        \"\"\" Read query from etl object and print in dataframe\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            rows = self.sqlConn.executeReadQueries(query)\n",
    "            df = pd.DataFrame(rows, columns=df_columns)\n",
    "            print(df.to_markdown())\n",
    "        except Exception as ex:\n",
    "            print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10285f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLConnector:\n",
    "    \"\"\" This class is to interact with sqlite3 database \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize all required variables\n",
    "        \"\"\"\n",
    "        # lets keep it private for now\n",
    "        self._connection = None\n",
    "        self._cursor = None\n",
    "        self._dbName = 'tubing.db'\n",
    "        \n",
    "    def __del__(self):\n",
    "        \"\"\" Destructor function - destructor will close the dangling connections on destroy\n",
    "        \"\"\"\n",
    "        if self._cursor:\n",
    "            self._cursor.close()\n",
    "            self._cursor = None\n",
    "        if self._connection:\n",
    "            self._connection.close()\n",
    "            self._connection = None\n",
    "            \n",
    "    def connectToDB(self):\n",
    "        \"\"\" Connects to database\n",
    "        \"\"\"\n",
    "        if self._connection is None:\n",
    "            try:\n",
    "                self._connection = sqlite3.connect(self._dbName)\n",
    "                self._cursor = self._connection.cursor()\n",
    "                return True\n",
    "            except Error as e:\n",
    "                print(e)\n",
    "                \n",
    "                \n",
    "        return False\n",
    "    \n",
    "    def disconnectFromDB(self):\n",
    "        \"\"\" Disconnects from database\n",
    "        \"\"\"\n",
    "        if self._cursor:\n",
    "            self._cursor.close()\n",
    "            self._cursor = None\n",
    "        if self._connection:\n",
    "            self._connection.close()\n",
    "            self._connection = None\n",
    "    \n",
    "    def createTables(self):\n",
    "        \"\"\" Create default tables \n",
    "        \"\"\"\n",
    "        if self._connection is None or self._cursor is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # this is expensive and should be avoided; but since we can \n",
    "            # re-run the playbook, drop the old tables \n",
    "            self._cursor.execute(sql_queries.DROP_WEATHER_TABLE)\n",
    "            self._cursor.execute(sql_queries.DROP_LOCATION_TABLE)\n",
    "            self._cursor.execute(sql_queries.DROP_DATE_TABLE)\n",
    "            self._cursor.execute(sql_queries.DROP_TRANSACTIONS_TABLE)\n",
    "            \n",
    "            # create tables\n",
    "            self._cursor.execute(sql_queries.CREATE_DATE_TABLE)\n",
    "            self._cursor.execute(sql_queries.CREATE_LOCATION_TABLE)\n",
    "            self._cursor.execute(sql_queries.CREATE_WEATHER_TABLE)\n",
    "            self._cursor.execute(sql_queries.CREATE_TRANSACTIONS_TABLE)\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "        return True\n",
    "            \n",
    "            \n",
    "    def getConnectionHandle(self):\n",
    "        \"\"\" Gets the SQL connector object\n",
    "        \"\"\"\n",
    "        return self._connection\n",
    "    \n",
    "    def executeWriteQueries(self, df, table):\n",
    "        \"\"\" Flushes the dataframe to database tables.\n",
    "        \"\"\"\n",
    "        if self._connection is None or self._cursor is None:\n",
    "            return False\n",
    "                \n",
    "        try:\n",
    "            # dont drop the table with multiple transactions data writes - hence if_exists is append\n",
    "            # chunksize is set to 1000 for batch flush\n",
    "            # skip df index in db writes - index is False\n",
    "            nof_writes = df.to_sql(table, self._connection, if_exists='append', index=False, chunksize=1000)\n",
    "            self._connection.commit()\n",
    "            return nof_writes\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "        return 0\n",
    "        \n",
    "    def executeReadQueries(self, query):\n",
    "        \"\"\" Executes read queries\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        if self._connection is None:\n",
    "            return rows\n",
    "        \n",
    "        try:\n",
    "            cursor = self._connection.cursor()\n",
    "            cursor.execute(query)\n",
    "            rows = cursor.fetchall()\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "        return rows\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56105ee3",
   "metadata": {},
   "source": [
    "# <ins>SQL Connector object</ins>\n",
    "\n",
    "- This will initialize tubing database\n",
    "- It will also execute DDL queries for different tables\n",
    "- For the sake of ease of use, this drops tables if they are already there to allow multiple runs of playbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbaa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a connection handle object\n",
    "connector = SQLConnector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a1e9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to database\n",
    "connector.connectToDB()\n",
    "\n",
    "# create tables\n",
    "connector.createTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefaf677",
   "metadata": {},
   "source": [
    "# <ins>ETL object</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44aa6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL object\n",
    "etl = ETL(connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ddf0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fire ETL - parse the files\n",
    "etl.parseFilesToDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "196f7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create date dataframe\n",
    "# this creates a dataframe with all the dates and their corresponding days\n",
    "# etc which will be joined with holiday data\n",
    "etl.createDateDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe58620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Preliminary Analysis of Transactions Data (CSV)\n",
      "-----------------------------------------------\n",
      "\n",
      "|    |   location_id | date       |   transaction_id | profit   | file_name                    |\n",
      "|---:|--------------:|:-----------|-----------------:|:---------|:-----------------------------|\n",
      "|  0 |           008 | 01/02/2019 |              001 | $26.89   | transactions_008_system1.csv |\n",
      "|  1 |           008 | 01/02/2019 |              002 | $24.74   | transactions_008_system1.csv |\n",
      "|  2 |           008 | 01/02/2019 |              003 | $31.36   | transactions_008_system1.csv |\n",
      "|  3 |           008 | 01/02/2019 |              004 | $27.06   | transactions_008_system1.csv |\n",
      "|  4 |           008 | 01/02/2019 |              005 | $29.51   | transactions_008_system1.csv |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8506 entries, 0 to 8505\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   location_id     8506 non-null   object\n",
      " 1   date            8506 non-null   object\n",
      " 2   transaction_id  8506 non-null   object\n",
      " 3   profit          8506 non-null   object\n",
      " 4   file_name       8506 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 332.4+ KB\n",
      "\n",
      "=====> There are total 8506 rows in Transactions Data (CSV)\n",
      "\n",
      "=====> There are 0 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "Preliminary Analysis of Transactions Data  (TEXT)\n",
      "-----------------------------------------------\n",
      "\n",
      "|    |   location_id | date       |   transaction_id |   profit | file_name                    |\n",
      "|---:|--------------:|:-----------|-----------------:|---------:|:-----------------------------|\n",
      "|  0 |           003 | 01-02-2019 |              001 |    26.74 | transactions_003_system2.txt |\n",
      "|  1 |           003 | 01-02-2019 |              002 |    23.26 | transactions_003_system2.txt |\n",
      "|  2 |           003 | 01-02-2019 |              003 |    28.66 | transactions_003_system2.txt |\n",
      "|  3 |           003 | 01-02-2019 |              004 |    29.69 | transactions_003_system2.txt |\n",
      "|  4 |           003 | 01-02-2019 |              005 |    25.55 | transactions_003_system2.txt |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 42167 entries, 0 to 9497\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   location_id     42167 non-null  object\n",
      " 1   date            42167 non-null  object\n",
      " 2   transaction_id  42167 non-null  object\n",
      " 3   profit          42151 non-null  object\n",
      " 4   file_name       42167 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.9+ MB\n",
      "\n",
      "=====> There are total 42167 rows in Transactions Data  (TEXT)\n",
      "\n",
      "=====> There are 1 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "Preliminary Analysis of Transactions Data (JSON)\n",
      "-----------------------------------------------\n",
      "\n",
      "|    |   location_id | date                |   transaction_id |   profit | file_name                     |\n",
      "|---:|--------------:|:--------------------|-----------------:|---------:|:------------------------------|\n",
      "|  0 |           001 | 2019-01-02 00:00:00 |              001 |    25.14 | transactions_001_system3.json |\n",
      "|  1 |           001 | 2019-01-02 00:00:00 |              002 |    21.69 | transactions_001_system3.json |\n",
      "|  2 |           001 | 2019-01-02 00:00:00 |              003 |    24.74 | transactions_001_system3.json |\n",
      "|  3 |           001 | 2019-01-02 00:00:00 |              004 |    23.08 | transactions_001_system3.json |\n",
      "|  4 |           001 | 2019-01-02 00:00:00 |              005 |    23.24 | transactions_001_system3.json |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 34674 entries, 0 to 7489\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   location_id     34674 non-null  object        \n",
      " 1   date            34674 non-null  datetime64[ns]\n",
      " 2   transaction_id  34674 non-null  object        \n",
      " 3   profit          34663 non-null  float64       \n",
      " 4   file_name       34674 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(3)\n",
      "memory usage: 1.6+ MB\n",
      "\n",
      "=====> There are total 34674 rows in Transactions Data (JSON)\n",
      "\n",
      "=====> There are 1 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "Preliminary Analysis of Weather Data\n",
      "-----------------------------------------------\n",
      "\n",
      "|    |   location_id | date       |   temperature |   pressure |   humidity | cloudy   | precipitation   | file_name        |\n",
      "|---:|--------------:|:-----------|--------------:|-----------:|-----------:|:---------|:----------------|:-----------------|\n",
      "|  0 |           003 | 2020-01-22 |         18.14 |   1035.06  |       0.44 | True     | False           | weather_data.csv |\n",
      "|  1 |           008 | 2022-01-29 |         14.36 |   1027.25  |       0.95 | True     | False           | weather_data.csv |\n",
      "|  2 |           005 | 2021-11-28 |         35.42 |    994.695 |       0.37 | False    | False           | weather_data.csv |\n",
      "|  3 |           006 | 2021-10-12 |         37.94 |   1003.84  |       0.11 | True     | True            | weather_data.csv |\n",
      "|  4 |           002 | 2020-12-03 |         23.36 |   1027.48  |       0.6  | False    | False           | weather_data.csv |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17278 entries, 0 to 17277\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   location_id    17278 non-null  object \n",
      " 1   date           17278 non-null  object \n",
      " 2   temperature    17278 non-null  float64\n",
      " 3   pressure       17278 non-null  float64\n",
      " 4   humidity       17278 non-null  float64\n",
      " 5   cloudy         17278 non-null  bool   \n",
      " 6   precipitation  17278 non-null  bool   \n",
      " 7   file_name      17278 non-null  object \n",
      "dtypes: bool(2), float64(3), object(3)\n",
      "memory usage: 843.8+ KB\n",
      "\n",
      "=====> There are total 17278 rows in Weather Data\n",
      "\n",
      "=====> There are 0 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "Preliminary Analysis of Location Data\n",
      "-----------------------------------------------\n",
      "\n",
      "|    |   location_id |   population |   elevation | file_name         |\n",
      "|---:|--------------:|-------------:|------------:|:------------------|\n",
      "|  0 |           001 |        18428 |         375 | location_data.csv |\n",
      "|  1 |           002 |        32926 |         274 | location_data.csv |\n",
      "|  2 |           003 |        74138 |         505 | location_data.csv |\n",
      "|  3 |           004 |        14255 |         360 | location_data.csv |\n",
      "|  4 |           005 |        12686 |         386 | location_data.csv |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13 entries, 0 to 12\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   location_id  13 non-null     object\n",
      " 1   population   13 non-null     int64 \n",
      " 2   elevation    13 non-null     int64 \n",
      " 3   file_name    13 non-null     object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 544.0+ bytes\n",
      "\n",
      "=====> There are total 13 rows in Location Data\n",
      "\n",
      "=====> There are 0 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "Preliminary Analysis of Holiday Data\n",
      "-----------------------------------------------\n",
      "\n",
      "|    | date                | holiday   |\n",
      "|---:|:--------------------|:----------|\n",
      "|  0 | 2019-01-01 00:00:00 | True      |\n",
      "|  1 | 2019-05-27 00:00:00 | True      |\n",
      "|  2 | 2019-07-04 00:00:00 | True      |\n",
      "|  3 | 2019-09-02 00:00:00 | True      |\n",
      "|  4 | 2019-11-28 00:00:00 | True      |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   date     22 non-null     datetime64[ns]\n",
      " 1   holiday  22 non-null     bool          \n",
      "dtypes: bool(1), datetime64[ns](1)\n",
      "memory usage: 326.0 bytes\n",
      "\n",
      "=====> There are total 22 rows in Holiday Data\n",
      "\n",
      "=====> There are 0 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "Preliminary Analysis of Date Data (Derived)\n",
      "-----------------------------------------------\n",
      "\n",
      "|    | date                |   day |   day_of_week |   month |   year | holiday   |\n",
      "|---:|:--------------------|------:|--------------:|--------:|-------:|:----------|\n",
      "|  0 | 2019-01-01 00:00:00 |     1 |             1 |       1 |   2019 | True      |\n",
      "|  1 | 2019-01-02 00:00:00 |     2 |             2 |       1 |   2019 | False     |\n",
      "|  2 | 2019-01-03 00:00:00 |     3 |             3 |       1 |   2019 | False     |\n",
      "|  3 | 2019-01-04 00:00:00 |     4 |             4 |       1 |   2019 | False     |\n",
      "|  4 | 2019-01-05 00:00:00 |     5 |             5 |       1 |   2019 | False     |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   date         1400 non-null   datetime64[ns]\n",
      " 1   day          1400 non-null   int32         \n",
      " 2   day_of_week  1400 non-null   int32         \n",
      " 3   month        1400 non-null   int32         \n",
      " 4   year         1400 non-null   int32         \n",
      " 5   holiday      1400 non-null   bool          \n",
      "dtypes: bool(1), datetime64[ns](1), int32(4)\n",
      "memory usage: 34.3 KB\n",
      "\n",
      "=====> There are total 1400 rows in Date Data (Derived)\n",
      "\n",
      "=====> There are 0 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "=====> Location ID 013 has no corresponding transactions data\n",
      "\n",
      "\n",
      "=====> Location ID 011 has no corresponding transactions data\n",
      "\n",
      "\n",
      "=====> Location ID 012 has no corresponding transactions data\n",
      "\n",
      "=====> There are 3995 records in weather data for locations with no transactions data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====> There are 390 such records in transactions data with missing weather data\n",
      "\n",
      "=====> There are 0 records in transaction data for dates which are major holidays\n"
     ]
    }
   ],
   "source": [
    "# if prelim analysis is required\n",
    "if ANALYZE:    \n",
    "    print(\"-----------------------------------------------\\n\")\n",
    "    etl.prelimAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dacde4",
   "metadata": {},
   "source": [
    "# <ins>Transformation</ins>\n",
    "\n",
    "- Here we need to deal with all the missing values/NULL values/NA (we will drop them for now from transaction data)\n",
    "- Date formatting is generalized for database insertion as the raw data has date in different formats\n",
    "- Float values for temperature, pressure are rounded to 2 decimal points.\n",
    "- Profit values are not generic across raw data, with some systems generating data with \\$ in the value and different formats for negative values. This is handled with regex pattern handler.\n",
    "- The dataframe is adjusted as per the column expectations of the database tables (as per ER diagram) for easy loading of data.\n",
    "\n",
    "\n",
    "Optionally save the processed data into files as well (if configured). If clean data is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aec751ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records before transformation = 8506 for Transactions Data (CSV)\n",
      "No of null rows dropped = 0\n",
      "No of duplicate rows dropped = 0\n",
      "Total number of records after transformation = 8506 for Transactions Data (CSV)\n",
      "Summary of Transactions Data (CSV)\n",
      "     transaction_id location_id        date  profit\n",
      "0               001         008  2019-01-02   26.89\n",
      "1               002         008  2019-01-02   24.74\n",
      "2               003         008  2019-01-02   31.36\n",
      "3               004         008  2019-01-02   27.60\n",
      "4               005         008  2019-01-02   29.51\n",
      "...             ...         ...         ...     ...\n",
      "8501            009         008  2022-09-11   26.60\n",
      "8502            010         008  2022-09-11   25.60\n",
      "8503            011         008  2022-09-11   28.38\n",
      "8504            012         008  2022-09-11   29.19\n",
      "8505            013         008  2022-09-11   25.86\n",
      "\n",
      "[8506 rows x 4 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8506 entries, 0 to 8505\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   transaction_id  8506 non-null   object \n",
      " 1   location_id     8506 non-null   object \n",
      " 2   date            8506 non-null   object \n",
      " 3   profit          8506 non-null   float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 265.9+ KB\n",
      "Total number of records before transformation = 42167 for Transactions Data  (TEXT)\n",
      "No of null rows dropped = 16\n",
      "No of duplicate rows dropped = 0\n",
      "Total number of records after transformation = 42151 for Transactions Data  (TEXT)\n",
      "Summary of Transactions Data  (TEXT)\n",
      "     transaction_id location_id        date  profit\n",
      "0               001         003  2019-01-02   26.74\n",
      "1               002         003  2019-01-02   23.26\n",
      "2               003         003  2019-01-02   28.66\n",
      "3               004         003  2019-01-02   29.69\n",
      "4               005         003  2019-01-02   25.55\n",
      "...             ...         ...         ...     ...\n",
      "9493            011         009  2022-09-11   26.11\n",
      "9494            012         009  2022-09-11   24.57\n",
      "9495            013         009  2022-09-11   23.45\n",
      "9496            014         009  2022-09-11   23.18\n",
      "9497            015         009  2022-09-11   23.68\n",
      "\n",
      "[42151 rows x 4 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 42151 entries, 0 to 9497\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   transaction_id  42151 non-null  object \n",
      " 1   location_id     42151 non-null  object \n",
      " 2   date            42151 non-null  object \n",
      " 3   profit          42151 non-null  float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 1.6+ MB\n",
      "Total number of records before transformation = 34674 for Transactions Data (JSON)\n",
      "No of null rows dropped = 11\n",
      "No of duplicate rows dropped = 0\n",
      "Total number of records after transformation = 34663 for Transactions Data (JSON)\n",
      "Summary of Transactions Data (JSON)\n",
      "     transaction_id location_id        date  profit\n",
      "0               001         001  2019-01-02   25.14\n",
      "1               002         001  2019-01-02   21.69\n",
      "2               003         001  2019-01-02   24.74\n",
      "3               004         001  2019-01-02   23.08\n",
      "4               005         001  2019-01-02   23.24\n",
      "...             ...         ...         ...     ...\n",
      "7485            008         010  2022-09-11   29.76\n",
      "7486            009         010  2022-09-11   31.18\n",
      "7487            010         010  2022-09-11   28.17\n",
      "7488            011         010  2022-09-11   30.15\n",
      "7489            012         010  2022-09-11   31.27\n",
      "\n",
      "[34663 rows x 4 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 34663 entries, 0 to 7489\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   transaction_id  34663 non-null  object \n",
      " 1   location_id     34663 non-null  object \n",
      " 2   date            34663 non-null  object \n",
      " 3   profit          34663 non-null  float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 1.3+ MB\n",
      "Total number of records before transformation = 17278 for Weather Data\n",
      "No of null rows dropped = 0\n",
      "No of duplicate rows dropped = 0\n",
      "Total number of records after transformation = 17278 for Weather Data\n",
      "Summary of Weather Data\n",
      "             date location_id  temperature  pressure  humidity  cloudy  \\\n",
      "0      2020-01-22         003        18.14   1035.06      0.44    True   \n",
      "1      2022-01-29         008        14.36   1027.25      0.95    True   \n",
      "2      2021-11-28         005        35.42    994.69      0.37   False   \n",
      "3      2021-10-12         006        37.94   1003.84      0.11    True   \n",
      "4      2020-12-03         002        23.36   1027.48      0.60   False   \n",
      "...           ...         ...          ...       ...       ...     ...   \n",
      "17273  2022-07-03         006        78.98    966.82      0.88    True   \n",
      "17274  2020-02-20         006         9.68    971.95      0.53   False   \n",
      "17275  2020-10-02         010        41.18   1013.87      0.90   False   \n",
      "17276  2019-12-15         001        15.08    990.46      0.74   False   \n",
      "17277  2020-12-12         006        21.92    976.85      0.59    True   \n",
      "\n",
      "       precipitation  \n",
      "0              False  \n",
      "1              False  \n",
      "2              False  \n",
      "3               True  \n",
      "4              False  \n",
      "...              ...  \n",
      "17273           True  \n",
      "17274          False  \n",
      "17275          False  \n",
      "17276          False  \n",
      "17277           True  \n",
      "\n",
      "[17278 rows x 7 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17278 entries, 0 to 17277\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           17278 non-null  object \n",
      " 1   location_id    17278 non-null  object \n",
      " 2   temperature    17278 non-null  float64\n",
      " 3   pressure       17278 non-null  float64\n",
      " 4   humidity       17278 non-null  float64\n",
      " 5   cloudy         17278 non-null  bool   \n",
      " 6   precipitation  17278 non-null  bool   \n",
      "dtypes: bool(2), float64(3), object(2)\n",
      "memory usage: 708.8+ KB\n",
      "Total number of records before transformation = 13 for Location Data\n",
      "No of null rows dropped = 0\n",
      "No of duplicate rows dropped = 0\n",
      "Total number of records after transformation = 13 for Location Data\n",
      "Summary of Location Data\n",
      "   location_id  elevation  population\n",
      "0          001        375       18428\n",
      "1          002        274       32926\n",
      "2          003        505       74138\n",
      "3          004        360       14255\n",
      "4          005        386       12686\n",
      "5          006        435       86372\n",
      "6          007        186       13400\n",
      "7          008        398       52185\n",
      "8          009        350       13641\n",
      "9          010        266      425336\n",
      "10         011        398      121465\n",
      "11         012        436      196528\n",
      "12         013        310       68818\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13 entries, 0 to 12\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   location_id  13 non-null     object\n",
      " 1   elevation    13 non-null     int64 \n",
      " 2   population   13 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 440.0+ bytes\n",
      "Total number of records before transformation = 1400 for Date Data (Derived)\n",
      "Total number of records after transformation = 1400 for Date Data (Derived)\n",
      "Summary of Date Data (Derived)\n",
      "            date  day  day_of_week  month  year  holiday\n",
      "0     2019-01-01    1            1      1  2019     True\n",
      "1     2019-01-02    2            2      1  2019    False\n",
      "2     2019-01-03    3            3      1  2019    False\n",
      "3     2019-01-04    4            4      1  2019    False\n",
      "4     2019-01-05    5            5      1  2019    False\n",
      "...          ...  ...          ...    ...   ...      ...\n",
      "1395  2022-10-27   27            3     10  2022    False\n",
      "1396  2022-10-28   28            4     10  2022    False\n",
      "1397  2022-10-29   29            5     10  2022    False\n",
      "1398  2022-10-30   30            6     10  2022    False\n",
      "1399  2022-10-31   31            0     10  2022    False\n",
      "\n",
      "[1400 rows x 6 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   date         1400 non-null   object\n",
      " 1   day          1400 non-null   int32 \n",
      " 2   day_of_week  1400 non-null   int32 \n",
      " 3   month        1400 non-null   int32 \n",
      " 4   year         1400 non-null   int32 \n",
      " 5   holiday      1400 non-null   bool  \n",
      "dtypes: bool(1), int32(4), object(1)\n",
      "memory usage: 34.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# transform the parsed data for DB ingestion\n",
    "etl.transformData()\n",
    "\n",
    "# this is optional controlled by a variable\n",
    "etl.saveProcessedData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034de748",
   "metadata": {},
   "source": [
    "# <ins>Load the transformed data</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "551e326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 8506 rows to table transactions\n",
      "Wrote 42151 rows to table transactions\n",
      "Wrote 34663 rows to table transactions\n",
      "Wrote 17278 rows to table weather\n",
      "Wrote 13 rows to table location\n",
      "Wrote 1400 rows to table date\n"
     ]
    }
   ],
   "source": [
    "etl.ingestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff878898",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL stats: Writes: 104011, Reads: 0\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    print(f\"ETL stats: Writes: {etl.stats['writes']}, Reads: {etl.stats['reads']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b57762",
   "metadata": {},
   "source": [
    "# <ins>Suggestions for additional columns or changes (for database performance)</ins>\n",
    "\n",
    "- Storing ID's (_location_id_, _transaction_id_) as _integers_ is preferred over _varchar_ for improved performance, as integer comparisons are generally faster in database operations.\n",
    "- Creating surrogate keys for each table will help improve database performance for read/writes when any natural key has NULL values. Queries would also be faster when we join surrogate key's with other table surrogate key's.\n",
    "- _DATE_ table could have extra _dateid_ column of type _integer_ which would improve performance of read/writes for massive amounts of data.\n",
    "- The ER diagram in this task appears to be a Star schema. We could name the table names as _DimensionLocation_, _DimensionDate_, _FactTransactions_ etc for easier understanding.\n",
    "- _Views_ could be created for different tables to customize views for users and can also help with security authorizations per user for making Business decision. Retrieving data from such _views_ are generally faster as the fetch is restricted to a subset of rows and columns.\n",
    "- Proper Audit/logging/statistics mechanisms must be employed to audit when data was inserted with proper timestamps and all details.\n",
    "\n",
    "Following data if available can help in building enhanced reports:\n",
    "- Data about exact number of visitors (tourists), number of competitors in rental industry, amount of rental units per location, \\% of such rental reservations per day, profit margin per rental per location can further help in analyzing transaction data and identifying scope for profit increment.\n",
    "- Sales target values that can be set. This will be useful for comparing the actual profits and targets.\n",
    "- Daily expense and revenue data, useful for comparing different location performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4cecf",
   "metadata": {},
   "source": [
    "# <ins>Run the query</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d230306",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   location_id | date       |   temperature |   profit | profit_stmt   |   dod_profit |   roll_30d_profit |\n",
      "|---:|--------------:|:-----------|--------------:|---------:|:--------------|-------------:|------------------:|\n",
      "|  0 |           001 | 2019-01-02 |         16.88 |   430.96 | positive      |       nan    |            430.96 |\n",
      "|  1 |           001 | 2019-01-03 |         17.24 |   416.9  | positive      |        -3.26 |            847.86 |\n",
      "|  2 |           001 | 2019-01-04 |          9.32 |   280.16 | positive      |       -32.8  |           1128.02 |\n",
      "|  3 |           001 | 2019-01-05 |         20.12 |   415.98 | positive      |        48.48 |           1544    |\n",
      "|  4 |           001 | 2019-01-06 |         17.24 |   412.2  | positive      |        -0.91 |           1956.2  |\n",
      "|  5 |           001 | 2019-01-07 |         14.9  |   165.01 | positive      |       -59.97 |           2121.21 |\n",
      "|  6 |           001 | 2019-01-08 |          0    |   161.52 | positive      |        -2.12 |           2282.73 |\n",
      "|  7 |           001 | 2019-01-09 |         15.8  |   380.51 | positive      |       135.58 |           2663.24 |\n",
      "|  8 |           001 | 2019-01-10 |         20.12 |   398.51 | positive      |         4.73 |           3061.75 |\n",
      "|  9 |           001 | 2019-01-11 |         19.58 |   397.37 | positive      |        -0.29 |           3459.12 |\n",
      "| 10 |           001 | 2019-01-12 |         15.44 |   436.29 | positive      |         9.79 |           3895.41 |\n",
      "| 11 |           001 | 2019-01-13 |         16.52 |   490.31 | positive      |        12.38 |           4385.72 |\n",
      "| 12 |           001 | 2019-01-14 |          6.8  |   134.93 | positive      |       -72.48 |           4520.65 |\n",
      "| 13 |           001 | 2019-01-15 |         15.98 |   304.73 | positive      |       125.84 |           4825.38 |\n",
      "| 14 |           001 | 2019-01-16 |          8.78 |   177.72 | positive      |       -41.68 |           5003.1  |\n",
      "| 15 |           001 | 2019-01-17 |         12.38 |   163.11 | positive      |        -8.22 |           5166.21 |\n",
      "| 16 |           001 | 2019-01-18 |          8.24 |   152.42 | positive      |        -6.55 |           5318.63 |\n",
      "| 17 |           001 | 2019-01-19 |         10.4  |   398.71 | positive      |       161.59 |           5717.34 |\n",
      "| 18 |           001 | 2019-01-20 |         21.02 |   536.83 | positive      |        34.64 |           6254.17 |\n",
      "| 19 |           001 | 2019-01-21 |         12.2  |   237.06 | positive      |       -55.84 |           6491.23 |\n",
      "| 20 |           001 | 2019-01-22 |         13.82 |   134.73 | positive      |       -43.17 |           6625.96 |\n",
      "| 21 |           001 | 2019-01-23 |         21.2  |   393.78 | positive      |       192.27 |           7019.74 |\n",
      "| 22 |           001 | 2019-01-24 |         13.1  |   235.03 | positive      |       -40.31 |           7254.77 |\n",
      "| 23 |           001 | 2019-01-25 |         13.28 |   251.14 | positive      |         6.85 |           7505.91 |\n",
      "| 24 |           001 | 2019-01-26 |          6.8  |   392.57 | positive      |        56.32 |           7898.48 |\n",
      "| 25 |           001 | 2019-01-27 |          6.8  |   264.46 | positive      |       -32.63 |           8162.94 |\n",
      "| 26 |           001 | 2019-01-28 |         10.22 |   128.43 | positive      |       -51.44 |           8291.37 |\n",
      "| 27 |           001 | 2019-01-29 |         18.14 |   384.88 | positive      |       199.68 |           8676.25 |\n",
      "| 28 |           001 | 2019-01-30 |         16.7  |   378.91 | positive      |        -1.55 |           9055.16 |\n",
      "| 29 |           001 | 2019-01-31 |         23.36 |   361.27 | positive      |        -4.66 |           9416.43 |\n",
      "| 30 |           001 | 2019-02-01 |         22.1  |   207.38 | positive      |       -42.6  |           9192.85 |\n",
      "| 31 |           001 | 2019-02-02 |         12.2  |   220.19 | positive      |         6.18 |           8996.14 |\n",
      "| 32 |           001 | 2019-02-03 |         16.88 |   503.97 | positive      |       128.88 |           9219.95 |\n",
      "| 33 |           001 | 2019-02-04 |          0    |   325.44 | positive      |       -35.42 |           9129.41 |\n",
      "| 34 |           001 | 2019-02-05 |         15.08 |   343.75 | positive      |         5.63 |           9060.96 |\n",
      "| 35 |           001 | 2019-02-06 |         16.88 |   330.9  | positive      |        -3.74 |           9226.85 |\n",
      "| 36 |           001 | 2019-02-07 |         24.26 |   330.43 | positive      |        -0.14 |           9395.76 |\n",
      "| 37 |           001 | 2019-02-08 |         20.84 |   338.07 | positive      |         2.31 |           9353.32 |\n",
      "| 38 |           001 | 2019-02-09 |          8.42 |   324.77 | positive      |        -3.93 |           9279.58 |\n",
      "| 39 |           001 | 2019-02-10 |         14.54 |   274.18 | positive      |       -15.58 |           9156.39 |\n",
      "| 40 |           001 | 2019-02-11 |         23.54 |   176.68 | positive      |       -35.56 |           8896.78 |\n",
      "| 41 |           001 | 2019-02-12 |         24.62 |   310.8  | positive      |        75.91 |           8717.27 |\n",
      "| 42 |           001 | 2019-02-13 |          9.14 |   167.89 | positive      |       -45.98 |           8750.23 |\n",
      "| 43 |           001 | 2019-02-14 |          9.32 |   153.07 | positive      |        -8.83 |           8598.57 |\n",
      "| 44 |           001 | 2019-02-15 |         12.38 |    37.53 | positive      |       -75.48 |           8458.38 |\n",
      "| 45 |           001 | 2019-02-16 |         12.56 |   196.96 | positive      |       424.81 |           8492.23 |\n",
      "| 46 |           001 | 2019-02-17 |         10.76 |   177.54 | positive      |        -9.86 |           8517.35 |\n",
      "| 47 |           001 | 2019-02-18 |         20.12 |   148.46 | positive      |       -16.38 |           8267.1  |\n",
      "| 48 |           001 | 2019-02-19 |         16.7  |   162.04 | positive      |         9.15 |           7892.31 |\n",
      "| 49 |           001 | 2019-02-20 |          8.6  |    18.27 | positive      |       -88.73 |           7673.52 |\n"
     ]
    }
   ],
   "source": [
    "# Run the query\n",
    "etl.readData(sql_queries.READ_QUERY, sql_queries.READ_QUERY_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98270e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "etl.sqlConn.disconnectFromDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ee996",
   "metadata": {},
   "source": [
    "# <ins>Interesting observations and considerations for end-user</ins>\n",
    "- Location with _id: 004_ is not making significant profits for last three years of data as compared to other rental locations (observed from SQL queries).\n",
    "- Location _010,_008, 001 are the top three locations making higher profits as compared to other locations (observed from SQL queries) for the year 2020 and 2021.\n",
    "- December seems to be the most profitable month for many rental locations (observed from SQL queries).\n",
    "- Location _011, 012, 013_ don't have any transaction data, so they should not be taken into consideration during data report interpretation\n",
    "- Population & Elevation information for different rental locations are not significant contributers to transaction analysis.\n",
    "- Holiday season like eg: _December 26th 2021_ had good precipitation, inspite of that good profits were made, indicating enough demand for such activities inspite of adverse weather.\n",
    "- For the year 2022 with available data, July and August are most profitable months.\n",
    "- Due to incomplete weather data, our final report may have rows with NULL temperature values.\n",
    "- For days with less than daily profit of \\$100, precipitation seems to be the major factor for all locations.\n",
    "- Saturday is the most popular day with more profits for location: _010_. Friday is the least popular day in terms of profits for the year 2021 in the month of december."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa73571c",
   "metadata": {},
   "source": [
    "# <ins>Additional metrics of interest</ins>\n",
    "\n",
    "These can also be included in the reports\n",
    "- YoY or month-over-month growth/decline in profits aggregated by location and date.\n",
    "- YTD sales aggregated by location and date.\n",
    "- Include precipitation values in the report as it seems to be a contributing factor for rentals (impacts number of visitors)\n",
    "- Day of the week (Mon, Tue..) can also be added in the report which is more easier for analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c290b",
   "metadata": {},
   "source": [
    "# <ins>Future Scope:</ins>\n",
    "- The ETL extract function can be rate limited. It can parse certain number of files, process and flush it to database before parsing too many files at once and increasing memory consumption. \n",
    "- The processed files should be moved to a separate directory to avoid re-processing of such files (data which is already processed and ingested into database).\n",
    "- ETL caching can be employed for static data and less number of rows to avoid database reads/updates/inserts.\n",
    "- Some ETL metrics/statistics can be added to give more visibility into ETL processing.\n",
    "- Better exception handling with different exceptions caught seperately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
