{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7df590",
   "metadata": {},
   "source": [
    "# <ins>\"_3M 2024 CBG Data Engineering Internship Take-Home Coding Assessment_\"</ins>\n",
    "\n",
    "- A lightweight simple ETL system to parse files with set expectations, transform as per database expectations and ingest them.\n",
    "- ETL class is made generic for multiple ETL objects in case of multiprocessing ETL etc (currently just one instance). Same case for SQLConnector class and its objects as well.\n",
    "- Currently runs to completion but can be enhanced later on as mentioned in future scope section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "766e3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author: Meghana Hegde\n",
    "# 02/2024\n",
    "\n",
    "# bunch of libraries that will come handy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import sqlite3\n",
    "from   sqlite3 import Error\n",
    "from   collections import defaultdict \n",
    "import sql_queries\n",
    "\n",
    "# this is where all data is \n",
    "DATA_DIR = os.path.dirname(os.getcwd()) + '/data'\n",
    "\n",
    "# set this to False if cleaned/processed data need not be saved into files.\n",
    "PROCESSED_DATA_SAVE = False\n",
    "\n",
    "PROCESSED_DATA_DIR = os.getcwd() + '/PROCESSED'\n",
    "if not os.path.exists(PROCESSED_DATA_DIR) and PROCESSED_DATA_SAVE is True:\n",
    "    os.makedirs(PROCESSED_DATA_DIR)\n",
    "\n",
    "# set this to False if you need to reduce the output/ETL logging.\n",
    "DEBUG = False\n",
    "\n",
    "# set this to False if analysis is not required (prelim analysis dump etc)\n",
    "ANALYZE = True\n",
    "\n",
    "# pretty names for logging\n",
    "DF_MAP_KEY_TO_NAMES = {\n",
    "    'trans_csv' : 'Transactions Data (CSV)',\n",
    "    'trans_json': 'Transactions Data (JSON)',\n",
    "    'trans_txt' : 'Transactions Data  (TEXT)',\n",
    "    'weather'   : 'Weather Data',\n",
    "    'location'  : 'Location Data',\n",
    "    'holiday'   : 'Holiday Data',\n",
    "    'date'      : 'Date Data (Derived)'\n",
    "}\n",
    "\n",
    "# value = [[l1], [l2] [l3]]\n",
    "# first list is the list of columns on which duplicates/NA values need to be dropped before insertion\n",
    "# second list is the column mapper of the corresponding tables in sql db.\n",
    "# third list is the database table name it maps to.\n",
    "DF_MAP_KEY_TO_FIELDS = {\n",
    "    'trans'   : [['transaction_id', 'location_id', 'date', 'profit'], ['transaction_id', 'location_id', 'date', 'profit'], ['transactions']],\n",
    "    'location': [['location_id'], ['location_id', 'elevation', 'population'], ['location']],\n",
    "    'date'    : [[], ['date', 'day', 'day_of_week', 'month', 'year', 'holiday'], ['date']],\n",
    "    'weather' : [['location_id','date'], ['date', 'location_id', 'temperature', 'pressure', 'humidity', 'cloudy', 'precipitation'], ['weather']],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dfc70de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETL:\n",
    "    \"\"\" ETL class for extract, transform, ingest and some analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, connector):\n",
    "        \"\"\" Initialize a dictionary to keep all dataframes to be processed.\n",
    "            Initialize sqlite db connection etc\n",
    "        \"\"\"\n",
    "        self.dataFrameMap = {}\n",
    "        self.sqlConn      = connector\n",
    "        self.stats        = {'writes': 0, 'reads': 0}\n",
    "        \n",
    "    def parseFilesToDf(self):\n",
    "        \"\"\" A parser function that walks over all files in a data directory\n",
    "            and parses files of interest and stores them in dataframe dict of df's per type of data.\n",
    "            We are interested in files ending with csv, txt, json. \n",
    "            Also, files with transaction, weather, location keywords in filenames\n",
    "        \"\"\"\n",
    "        trans_csv_dfs  = []\n",
    "        trans_json_dfs = []\n",
    "        trans_txt_dfs  = []\n",
    "        weather_dfs    = []\n",
    "        location_dfs   = []\n",
    "        holiday_dfs    = []\n",
    "        \n",
    "        # lets try to walk over all files of interest.\n",
    "        try:\n",
    "            for file in os.listdir(DATA_DIR):\n",
    "                file_path = DATA_DIR + '/' + file\n",
    "                \n",
    "                # parse transactions files.\n",
    "                if file.startswith('transactions'):\n",
    "                    \n",
    "                    # get the extension of the file\n",
    "                    _, extension = os.path.splitext(file)\n",
    "                \n",
    "                    if extension == \".csv\":\n",
    "                        try:\n",
    "                            # transid, location id have leading 0's - parse as it is as string and not int.\n",
    "                            df_csv = pd.read_csv(file_path, dtype={'transaction_id': str, 'location_id': str})\n",
    "                        except Exception as ex:\n",
    "                            # we should probably log this so that file parsing errors are known\n",
    "                            continue\n",
    "                            \n",
    "                        # storing filename in df temporarily\n",
    "                        df_csv['file_name'] = file\n",
    "                        trans_csv_dfs.append(df_csv)\n",
    "                    \n",
    "                    elif extension == '.txt':\n",
    "                        try:\n",
    "                            df_txt = pd.read_csv(file_path, sep='\\t', dtype={'transaction_id': str, 'location_id': str})\n",
    "                        except Exception as ex:\n",
    "                            continue\n",
    "                        df_txt['file_name'] = file\n",
    "                        trans_txt_dfs.append(df_txt)\n",
    "                \n",
    "                    elif extension == '.json':\n",
    "                        try:\n",
    "                            df_json = pd.read_json(file_path, dtype={'transaction_id': str, 'location_id': str})\n",
    "                        except Exception as ex:\n",
    "                            continue\n",
    "                        df_json['file_name'] = file\n",
    "                        trans_json_dfs.append(df_json)\n",
    "                    else:\n",
    "                        print(\"Unsupported file %s\" %(file))\n",
    "                    \n",
    "                elif file.startswith('location'):\n",
    "                    # these will always be csv \n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, dtype={'location_id': str})\n",
    "                    except Exception as ex:\n",
    "                        continue\n",
    "                    df['file_name'] = file\n",
    "                    location_dfs.append(df)\n",
    "                \n",
    "                elif file.startswith('weather'):\n",
    "                    # these will always be csv\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, dtype={'location_id': str})\n",
    "                    except Exception as ex:\n",
    "                        continue\n",
    "                    df['file_name'] = file\n",
    "                    weather_dfs.append(df)\n",
    "                    \n",
    "                elif file.startswith('holiday'):\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                    except Exception as ex:\n",
    "                        continue\n",
    "                    holiday_dfs.append(df)\n",
    "                    \n",
    "            # concat all df's of similar type\n",
    "            self.dataFrameMap['trans_csv'] = pd.concat(trans_csv_dfs)\n",
    "            self.dataFrameMap['trans_txt'] = pd.concat(trans_txt_dfs)\n",
    "            self.dataFrameMap['trans_json'] = pd.concat(trans_json_dfs)\n",
    "            self.dataFrameMap['weather'] = pd.concat(weather_dfs)\n",
    "            self.dataFrameMap['location'] = pd.concat(location_dfs)\n",
    "            self.dataFrameMap['holiday']  = pd.concat(holiday_dfs)\n",
    "                    \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "    def prelimAnalysis(self):\n",
    "        \"\"\" Prints preliminary analysis of a data frame.\n",
    "        \"\"\"\n",
    "        merged_trans_df_list = []\n",
    "        count = 0\n",
    "        \n",
    "        for key, data_frame in self.dataFrameMap.items():\n",
    "            # text to print\n",
    "            text = DF_MAP_KEY_TO_NAMES[key]\n",
    "            print(f\"\\n\\nPreliminary Analysis of {text}\")\n",
    "            print(\"-----------------------------------------------\\n\")\n",
    "            \n",
    "            try:\n",
    "                if 'trans' in key:\n",
    "                    merged_trans_df_list.append(data_frame)\n",
    "                \n",
    "                # Top 5 rows to get overview of data\n",
    "                print(data_frame.head().to_markdown())\n",
    "\n",
    "                # check information about data whether data is null, how many rows are appended to dataframe\n",
    "                print(\"\\n=====> Information about data\")\n",
    "                data_frame.info()\n",
    "\n",
    "                # check how many rows there in original file? how many appended\n",
    "                print(f\"\\n=====> There are total {data_frame.shape[0]} rows in {text}\")\n",
    "\n",
    "                # Null check for any rows in dataframe\n",
    "                null_values = data_frame.columns[data_frame.isnull().any()]\n",
    "                total_nulls = []\n",
    "\n",
    "\n",
    "                # check columns which has null values present\n",
    "                for column in null_values:\n",
    "                    total_nulls.append(data_frame[column])\n",
    "\n",
    "                print(f\"\\n=====> There are {len(total_nulls)} columns which has NULL values\")\n",
    "                \n",
    "                # Check for duplicate rows\n",
    "                duplicates = data_frame[data_frame.duplicated()]\n",
    "\n",
    "                # Display the duplicate rows, if any\n",
    "                if not duplicates.empty:\n",
    "                    print(\"\\n=====> Following are Duplicate Rows:\")\n",
    "                    print(duplicates)\n",
    "                else:\n",
    "                    print(\"\\n=====> No duplicate rows found.\")\n",
    "                    \n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            merged_trans_df = pd.concat(merged_trans_df_list)\n",
    "            location_df = self.dataFrameMap['location']\n",
    "            weather_df = self.dataFrameMap['weather']\n",
    "\n",
    "            loc_ids = set(location_df['location_id'])\n",
    "            trans_loc_ids = set(merged_trans_df['location_id'])\n",
    "\n",
    "            # this is to find all such locations which dont have transaction data\n",
    "            missing_ids = loc_ids - trans_loc_ids\n",
    "\n",
    "            for id in missing_ids:\n",
    "                print(f\"\\n\\n=====> Location ID {id} has no corresponding transactions data\")\n",
    "        \n",
    "            # try to find weather data for irrelevant locations (location that dont have trans data)\n",
    "            # these rows are of no use for our analysis.\n",
    "            weather_ids = set(weather_df['location_id'])\n",
    "            for id in [i for i in weather_df['location_id']]:\n",
    "                if id in list(missing_ids):\n",
    "                    count += 1\n",
    "                    \n",
    "            print(f\"\\n=====> There are {count} records in weather data for irrelevant locations\")\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "                \n",
    "    def createDateDataFrame(self):\n",
    "        \"\"\" Create a date dateframe to load all values from January 2019 to October 2022 as mentioned in project\n",
    "            We need to derive date, day, day_of_week, month and year\n",
    "            To derive holiday column we need to left join with existing date dataframe with \n",
    "            holiday_data dataframe to fetch holiday column\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create a date range from Jan 2019 to Oct 2022 (you can change end date and make it current date)\n",
    "            date_start_end = pd.date_range(start='2019-01-01', end='2022-10-31', freq='D').strftime('%m/%d/%Y')\n",
    "\n",
    "            # Create a DataFrame with the date range\n",
    "            date_df = pd.DataFrame({'date': date_start_end})\n",
    "\n",
    "            date_df['date'] = pd.to_datetime(date_df['date'])\n",
    "\n",
    "            # Add a new column 'Day' to represent the day of the week\n",
    "            date_df['day'] = date_df['date'].dt.day\n",
    "            date_df['day_of_week'] = date_df['date'].dt.dayofweek\n",
    "            date_df['month'] = date_df['date'].dt.month\n",
    "            date_df['year'] = date_df['date'].dt.year\n",
    "\n",
    "            # Fetch Holidays data\n",
    "            holiday_df = self.dataFrameMap['holiday']\n",
    "            holiday_df['date'] = pd.to_datetime(holiday_df['date'])\n",
    "\n",
    "            # Left join of date with holidays_data on date to fetch holiday column\n",
    "            date_df = date_df.merge(holiday_df, how='left', on='date')\n",
    "\n",
    "            # If Holiday then True else False\n",
    "            date_df['holiday'] = date_df['holiday'].notna()\n",
    "\n",
    "            # update the dataFrameMap with these values\n",
    "            self.dataFrameMap['date'] = date_df\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "        \n",
    "    def transformData(self):\n",
    "        \"\"\" Does some data transformations based on dataframe type and fields to suit database needs\n",
    "            - Deals with duplicate rows, rows with N/A or NULL values\n",
    "            - Generalize data format of different columns as per database expectations.\n",
    "            - data cleaning for profit values in transactions etc\n",
    "        \"\"\"\n",
    "        for key, data_frame in self.dataFrameMap.items():\n",
    "            if key in ['holiday']:\n",
    "                # these wont go into DB\n",
    "                continue\n",
    "                \n",
    "            # this is the key to index the KEY TO FIELDS dict\n",
    "            subset_key = key.split(\"_\")[0]\n",
    "            \n",
    "            # pretty text for print's\n",
    "            text = DF_MAP_KEY_TO_NAMES[key]\n",
    "            \n",
    "            # columns in DF\n",
    "            df_columns = data_frame.columns.values.tolist()\n",
    "            \n",
    "            # columns in DB\n",
    "            db_columns = DF_MAP_KEY_TO_FIELDS[subset_key][1]\n",
    "            \n",
    "            # Retrieve rows before dropping\n",
    "            nof_rows_before = data_frame.shape[0]\n",
    "            if DEBUG:\n",
    "                print(f\"Total number of records before transformation = {nof_rows_before} for {text}\")\n",
    "\n",
    "            # Remove rows with null data based on a subset of columns\n",
    "            if len(DF_MAP_KEY_TO_FIELDS[subset_key][0]) > 0:\n",
    "                # consider a subset of columns - if they have null values drop it \n",
    "                data_frame = data_frame.dropna(subset=DF_MAP_KEY_TO_FIELDS[subset_key][0])\n",
    "                nof_rows_after = data_frame.shape[0]\n",
    "                if DEBUG:\n",
    "                    print(f\"No of null rows dropped = {nof_rows_before - nof_rows_after}\")\n",
    "\n",
    "            # Remove duplicates (This is a dimension tables)\n",
    "            nof_rows_before = data_frame.shape[0]\n",
    "            if len(DF_MAP_KEY_TO_FIELDS[subset_key][0]) > 0:\n",
    "                data_frame = data_frame.drop_duplicates(subset=DF_MAP_KEY_TO_FIELDS[subset_key][0])\n",
    "                nof_rows_after = data_frame.shape[0]\n",
    "                if DEBUG:\n",
    "                    print(f\"No of duplicate rows dropped = {nof_rows_before - nof_rows_after}\")\n",
    "                \n",
    "            # some transformations as per db needs\n",
    "            if 'date' in df_columns:\n",
    "                # standardize different formats that we have into one format\n",
    "                data_frame['date'] = pd.to_datetime(data_frame['date']).dt.strftime('%Y-%m-%d')\n",
    "            if 'temperature' in df_columns:\n",
    "                # keep upto 2 decimals\n",
    "                data_frame['temperature'] = data_frame['temperature'].round(2)\n",
    "            if 'pressure' in df_columns:\n",
    "                # keep upto 2 decimals\n",
    "                data_frame['pressure'] = data_frame['pressure'].round(2)\n",
    "            if 'profit' in df_columns:\n",
    "                # strip leading $ character, handle negative profit values in different format etc.\n",
    "                if data_frame['profit'].dtype == 'O':\n",
    "                    data_frame['profit'] = data_frame['profit'].str.replace('[$, 0]', '', regex=True)\n",
    "                    data_frame['profit'] = pd.to_numeric(data_frame['profit'].str.replace('[(]', '-', regex=True), errors='coerce')\n",
    "            \n",
    "            # Arrange column names as it is in ER diagram for DB insertion\n",
    "            self.dataFrameMap[key] = data_frame[db_columns]\n",
    "            \n",
    "            if DEBUG:\n",
    "                nof_rows_after = self.dataFrameMap[key].shape[0]\n",
    "                print(f\"Total number of records after transformation = {nof_rows_after} for {text}\")\n",
    "                print(f\"Summary of {text}\")\n",
    "                print(self.dataFrameMap[key])\n",
    "                self.dataFrameMap[key].info()\n",
    "                \n",
    "    def saveProcessedData(self):\n",
    "        \"\"\" Optionally save the processed data in cleaned files\n",
    "            Every iteration the files are truncated first and written\n",
    "        \"\"\"\n",
    "        \n",
    "        merged_df_list = []\n",
    "        \n",
    "        if PROCESSED_DATA_SAVE is False:\n",
    "            return\n",
    "        \n",
    "        for key in ['trans_json', 'trans_csv', 'trans_txt']:\n",
    "            # gather all transactions df's from 3 systems for transactions\n",
    "            merged_df_list.append(self.dataFrameMap[key])\n",
    "            \n",
    "        merged_df = pd.concat(merged_df_list)\n",
    "    \n",
    "        # flush all the dataframes into files (dont write column names to file)\n",
    "        try:\n",
    "            merged_df.to_csv(PROCESSED_DATA_DIR + '/transactions.csv', header=False)\n",
    "            self.dataFrameMap['location'].to_csv(PROCESSED_DATA_DIR + '/location.csv', header=False)\n",
    "            self.dataFrameMap['weather'].to_csv(PROCESSED_DATA_DIR + '/weather.csv', header=False)\n",
    "            self.dataFrameMap['date'].to_csv(PROCESSED_DATA_DIR + '/date.csv', header=False)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "    def ingestData(self):\n",
    "        \"\"\" Ingests the data in database\n",
    "        \"\"\"\n",
    "        \n",
    "        for key, data_frame in self.dataFrameMap.items():\n",
    "            try:\n",
    "                if key in ['holiday']:\n",
    "                    # these wont go into DB\n",
    "                    continue\n",
    "\n",
    "                # this is the key to index the KEY TO FIELDS dict\n",
    "                subset_key = key.split(\"_\")[0]\n",
    "\n",
    "                # pretty text\n",
    "                text = DF_MAP_KEY_TO_NAMES[key]\n",
    "\n",
    "                # table name\n",
    "                table = DF_MAP_KEY_TO_FIELDS[subset_key][2][0]\n",
    "                \n",
    "                # flush the dataframe to db\n",
    "                nof_writes = self.sqlConn.executeWriteQueries(data_frame, table)\n",
    "                \n",
    "                self.stats['writes'] += nof_writes\n",
    "                print(f\"Wrote {nof_writes} rows to table {table}\")\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                \n",
    "    def readData(self, query, df_columns):\n",
    "        \"\"\" Read query from etl object and print in dataframe\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            rows = self.sqlConn.executeReadQueries(query)\n",
    "            df = pd.DataFrame(rows, columns=df_columns)\n",
    "            print(df.to_markdown())\n",
    "        except Exception as ex:\n",
    "            print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10285f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLConnector:\n",
    "    \"\"\" This class is to interact with sqlite3 database \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize all required variables\n",
    "        \"\"\"\n",
    "        # lets keep it private for now\n",
    "        self._connection = None\n",
    "        self._cursor = None\n",
    "        self._dbName = 'tubing'\n",
    "        \n",
    "    def __del__(self):\n",
    "        \"\"\" Destructor function - destructor will close the dangling connections on destroy\n",
    "        \"\"\"\n",
    "        if self._cursor:\n",
    "            self._cursor.close()\n",
    "            self._cursor = None\n",
    "        if self._connection:\n",
    "            self._connection.close()\n",
    "            self._connection = None\n",
    "            \n",
    "    def connectToDB(self):\n",
    "        \"\"\" Connects to database\n",
    "        \"\"\"\n",
    "        if self._connection is None:\n",
    "            try:\n",
    "                self._connection = sqlite3.connect(self._dbName)\n",
    "                self._cursor = self._connection.cursor()\n",
    "                return True\n",
    "            except Error as e:\n",
    "                print(e)\n",
    "                \n",
    "                \n",
    "        return False\n",
    "    \n",
    "    def disconnectFromDB(self):\n",
    "        \"\"\" Disconnects from database\n",
    "        \"\"\"\n",
    "        if self._cursor:\n",
    "            self._cursor.close()\n",
    "            self._cursor = None\n",
    "        if self._connection:\n",
    "            self._connection.close()\n",
    "            self._connection = None\n",
    "    \n",
    "    def createTables(self):\n",
    "        \"\"\" Create default tables \n",
    "        \"\"\"\n",
    "        if self._connection is None or self._cursor is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # this is expensive and should be avoided; but since we can \n",
    "            # re-run the playbook, its best to drop the old tables \n",
    "            # and ensure unique constraint doesnt reject data\n",
    "            self._cursor.execute(sql_queries.DROP_WEATHER_TABLE)\n",
    "            self._cursor.execute(sql_queries.DROP_LOCATION_TABLE)\n",
    "            self._cursor.execute(sql_queries.DROP_DATE_TABLE)\n",
    "            self._cursor.execute(sql_queries.DROP_TRANSACTIONS_TABLE)\n",
    "            \n",
    "            # create tables\n",
    "            self._cursor.execute(sql_queries.CREATE_DATE_TABLE)\n",
    "            self._cursor.execute(sql_queries.CREATE_LOCATION_TABLE)\n",
    "            self._cursor.execute(sql_queries.CREATE_WEATHER_TABLE)\n",
    "            self._cursor.execute(sql_queries.CREATE_TRANSACTIONS_TABLE)\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "        return True\n",
    "            \n",
    "            \n",
    "    def getConnectionHandle(self):\n",
    "        \"\"\" Gets the SQL connector object\n",
    "        \"\"\"\n",
    "        return self._connection\n",
    "    \n",
    "    def executeWriteQueries(self, df, table):\n",
    "        \"\"\" Flushes the dataframe to database tables.\n",
    "        \"\"\"\n",
    "        if self._connection is None or self._cursor is None:\n",
    "            return False\n",
    "                \n",
    "        try:\n",
    "            # dont drop the table with multiple transactions data writes - hence if_exists is append\n",
    "            # chunksize is set to 1000 to stagger the batch flush\n",
    "            # Do not attempt to write dataframe index to db - so skip index\n",
    "            nof_writes = df.to_sql(table, self._connection, if_exists='append', index=False, chunksize=1000)\n",
    "            self._connection.commit()\n",
    "            return nof_writes\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "        return 0\n",
    "        \n",
    "    def executeReadQueries(self, query):\n",
    "        \"\"\" Executes read queries\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        if self._connection is None:\n",
    "            return rows\n",
    "        \n",
    "        try:\n",
    "            cursor = self._connection.cursor()\n",
    "            cursor.execute(query)\n",
    "            rows = cursor.fetchall()\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "        return rows\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56105ee3",
   "metadata": {},
   "source": [
    "# <ins>SQL Connector object</ins>\n",
    "\n",
    "- this will initialize tubing database\n",
    "- it will also execute DDL queries for different tables\n",
    "- for the sake of ease of use, this drops tables if they are already there to allow multiple rules of playbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbaa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a connection handle object\n",
    "connector = SQLConnector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a1e9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to database\n",
    "connector.connectToDB()\n",
    "\n",
    "# create tables\n",
    "connector.createTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefaf677",
   "metadata": {},
   "source": [
    "# <ins>ETL object</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44aa6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL object\n",
    "etl = ETL(connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ddf0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fire ETL - parse the files\n",
    "etl.parseFilesToDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe58620",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Preliminary Analysis of Transactions Data (CSV)\n",
      "-----------------------------------------------\n",
      "\n",
      "|    |   location_id | date       |   transaction_id | profit   | file_name                    |\n",
      "|---:|--------------:|:-----------|-----------------:|:---------|:-----------------------------|\n",
      "|  0 |           008 | 01/02/2019 |              001 | $26.89   | transactions_008_system1.csv |\n",
      "|  1 |           008 | 01/02/2019 |              002 | $24.74   | transactions_008_system1.csv |\n",
      "|  2 |           008 | 01/02/2019 |              003 | $31.36   | transactions_008_system1.csv |\n",
      "|  3 |           008 | 01/02/2019 |              004 | $27.06   | transactions_008_system1.csv |\n",
      "|  4 |           008 | 01/02/2019 |              005 | $29.51   | transactions_008_system1.csv |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8506 entries, 0 to 8505\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   location_id     8506 non-null   object\n",
      " 1   date            8506 non-null   object\n",
      " 2   transaction_id  8506 non-null   object\n",
      " 3   profit          8506 non-null   object\n",
      " 4   file_name       8506 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 332.4+ KB\n",
      "\n",
      "=====> There are total 8506 rows in Transactions Data (CSV)\n",
      "\n",
      "=====> There are 0 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "Preliminary Analysis of Transactions Data  (TEXT)\n",
      "-----------------------------------------------\n",
      "\n",
      "|    |   location_id | date       |   transaction_id |   profit | file_name                    |\n",
      "|---:|--------------:|:-----------|-----------------:|---------:|:-----------------------------|\n",
      "|  0 |           003 | 01-02-2019 |              001 |    26.74 | transactions_003_system2.txt |\n",
      "|  1 |           003 | 01-02-2019 |              002 |    23.26 | transactions_003_system2.txt |\n",
      "|  2 |           003 | 01-02-2019 |              003 |    28.66 | transactions_003_system2.txt |\n",
      "|  3 |           003 | 01-02-2019 |              004 |    29.69 | transactions_003_system2.txt |\n",
      "|  4 |           003 | 01-02-2019 |              005 |    25.55 | transactions_003_system2.txt |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 42167 entries, 0 to 9497\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   location_id     42167 non-null  object\n",
      " 1   date            42167 non-null  object\n",
      " 2   transaction_id  42167 non-null  object\n",
      " 3   profit          42151 non-null  object\n",
      " 4   file_name       42167 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.9+ MB\n",
      "\n",
      "=====> There are total 42167 rows in Transactions Data  (TEXT)\n",
      "\n",
      "=====> There are 1 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "Preliminary Analysis of Transactions Data (JSON)\n",
      "-----------------------------------------------\n",
      "\n",
      "|    |   location_id | date                |   transaction_id |   profit | file_name                     |\n",
      "|---:|--------------:|:--------------------|-----------------:|---------:|:------------------------------|\n",
      "|  0 |           001 | 2019-01-02 00:00:00 |              001 |    25.14 | transactions_001_system3.json |\n",
      "|  1 |           001 | 2019-01-02 00:00:00 |              002 |    21.69 | transactions_001_system3.json |\n",
      "|  2 |           001 | 2019-01-02 00:00:00 |              003 |    24.74 | transactions_001_system3.json |\n",
      "|  3 |           001 | 2019-01-02 00:00:00 |              004 |    23.08 | transactions_001_system3.json |\n",
      "|  4 |           001 | 2019-01-02 00:00:00 |              005 |    23.24 | transactions_001_system3.json |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 34674 entries, 0 to 7489\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   location_id     34674 non-null  object        \n",
      " 1   date            34674 non-null  datetime64[ns]\n",
      " 2   transaction_id  34674 non-null  object        \n",
      " 3   profit          34663 non-null  float64       \n",
      " 4   file_name       34674 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(3)\n",
      "memory usage: 1.6+ MB\n",
      "\n",
      "=====> There are total 34674 rows in Transactions Data (JSON)\n",
      "\n",
      "=====> There are 1 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "Preliminary Analysis of Weather Data\n",
      "-----------------------------------------------\n",
      "\n",
      "|    |   location_id | date       |   temperature |   pressure |   humidity | cloudy   | precipitation   | file_name        |\n",
      "|---:|--------------:|:-----------|--------------:|-----------:|-----------:|:---------|:----------------|:-----------------|\n",
      "|  0 |           003 | 2020-01-22 |         18.14 |   1035.06  |       0.44 | True     | False           | weather_data.csv |\n",
      "|  1 |           008 | 2022-01-29 |         14.36 |   1027.25  |       0.95 | True     | False           | weather_data.csv |\n",
      "|  2 |           005 | 2021-11-28 |         35.42 |    994.695 |       0.37 | False    | False           | weather_data.csv |\n",
      "|  3 |           006 | 2021-10-12 |         37.94 |   1003.84  |       0.11 | True     | True            | weather_data.csv |\n",
      "|  4 |           002 | 2020-12-03 |         23.36 |   1027.48  |       0.6  | False    | False           | weather_data.csv |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17278 entries, 0 to 17277\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   location_id    17278 non-null  object \n",
      " 1   date           17278 non-null  object \n",
      " 2   temperature    17278 non-null  float64\n",
      " 3   pressure       17278 non-null  float64\n",
      " 4   humidity       17278 non-null  float64\n",
      " 5   cloudy         17278 non-null  bool   \n",
      " 6   precipitation  17278 non-null  bool   \n",
      " 7   file_name      17278 non-null  object \n",
      "dtypes: bool(2), float64(3), object(3)\n",
      "memory usage: 843.8+ KB\n",
      "\n",
      "=====> There are total 17278 rows in Weather Data\n",
      "\n",
      "=====> There are 0 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "Preliminary Analysis of Location Data\n",
      "-----------------------------------------------\n",
      "\n",
      "|    |   location_id |   population |   elevation | file_name         |\n",
      "|---:|--------------:|-------------:|------------:|:------------------|\n",
      "|  0 |           001 |        18428 |         375 | location_data.csv |\n",
      "|  1 |           002 |        32926 |         274 | location_data.csv |\n",
      "|  2 |           003 |        74138 |         505 | location_data.csv |\n",
      "|  3 |           004 |        14255 |         360 | location_data.csv |\n",
      "|  4 |           005 |        12686 |         386 | location_data.csv |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13 entries, 0 to 12\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   location_id  13 non-null     object\n",
      " 1   population   13 non-null     int64 \n",
      " 2   elevation    13 non-null     int64 \n",
      " 3   file_name    13 non-null     object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 544.0+ bytes\n",
      "\n",
      "=====> There are total 13 rows in Location Data\n",
      "\n",
      "=====> There are 0 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "Preliminary Analysis of Holiday Data\n",
      "-----------------------------------------------\n",
      "\n",
      "|    | date       | holiday   |\n",
      "|---:|:-----------|:----------|\n",
      "|  0 | 2019-01-01 | True      |\n",
      "|  1 | 2019-05-27 | True      |\n",
      "|  2 | 2019-07-04 | True      |\n",
      "|  3 | 2019-09-02 | True      |\n",
      "|  4 | 2019-11-28 | True      |\n",
      "\n",
      "=====> Information about data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   date     22 non-null     object\n",
      " 1   holiday  22 non-null     bool  \n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 326.0+ bytes\n",
      "\n",
      "=====> There are total 22 rows in Holiday Data\n",
      "\n",
      "=====> There are 0 columns which has NULL values\n",
      "\n",
      "=====> No duplicate rows found.\n",
      "\n",
      "\n",
      "=====> Location ID 012 has no corresponding transactions data\n",
      "\n",
      "\n",
      "=====> Location ID 013 has no corresponding transactions data\n",
      "\n",
      "\n",
      "=====> Location ID 011 has no corresponding transactions data\n",
      "\n",
      "=====> There are 3995 records in weather data for irrelevant locations\n"
     ]
    }
   ],
   "source": [
    "# if prelim analysis is required\n",
    "if ANALYZE:    \n",
    "    print(\"-----------------------------------------------\\n\")\n",
    "    etl.prelimAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1ed1d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create date dataframe\n",
    "# this creates a dataframe with all the dates and their corresponding days\n",
    "# etc which will be joined with holiday data\n",
    "etl.createDateDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aec751ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the parsed data for DB ingestion\n",
    "etl.transformData()\n",
    "\n",
    "# this is optional controlled by a variable\n",
    "etl.saveProcessedData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034de748",
   "metadata": {},
   "source": [
    "# <ins>Load the transformed data</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "551e326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 8506 rows to table transactions\n",
      "Wrote 42151 rows to table transactions\n",
      "Wrote 34663 rows to table transactions\n",
      "Wrote 17278 rows to table weather\n",
      "Wrote 13 rows to table location\n",
      "Wrote 1400 rows to table date\n"
     ]
    }
   ],
   "source": [
    "etl.ingestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff878898",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(f\"ETL stats: Writes: {etl.stats['writes']}, Reads: {etl.stats['reads']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4cecf",
   "metadata": {},
   "source": [
    "# <ins>Run the query</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d230306",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   location_id | date       |   temperature |   daily_sum_proft | income_stmt   |   percent_change |    roll |\n",
      "|---:|--------------:|:-----------|--------------:|------------------:|:--------------|-----------------:|--------:|\n",
      "|  0 |           001 | 2019-01-02 |         16.88 |            430.96 | positive      |           nan    |  430.96 |\n",
      "|  1 |           001 | 2019-01-03 |         17.24 |            416.9  | positive      |            -3.26 |  847.86 |\n",
      "|  2 |           001 | 2019-01-04 |          9.32 |            280.16 | positive      |           -32.8  | 1128.02 |\n",
      "|  3 |           001 | 2019-01-05 |         20.12 |            415.98 | positive      |            48.48 | 1544    |\n",
      "|  4 |           001 | 2019-01-06 |         17.24 |            412.2  | positive      |            -0.91 | 1956.2  |\n",
      "|  5 |           001 | 2019-01-07 |         14.9  |            165.01 | positive      |           -59.97 | 2121.21 |\n",
      "|  6 |           001 | 2019-01-08 |          0    |            161.52 | positive      |            -2.12 | 2282.73 |\n",
      "|  7 |           001 | 2019-01-09 |         15.8  |            380.51 | positive      |           135.58 | 2663.24 |\n",
      "|  8 |           001 | 2019-01-10 |         20.12 |            398.51 | positive      |             4.73 | 3061.75 |\n",
      "|  9 |           001 | 2019-01-11 |         19.58 |            397.37 | positive      |            -0.29 | 3459.12 |\n",
      "| 10 |           001 | 2019-01-12 |         15.44 |            436.29 | positive      |             9.79 | 3895.41 |\n",
      "| 11 |           001 | 2019-01-13 |         16.52 |            490.31 | positive      |            12.38 | 4385.72 |\n",
      "| 12 |           001 | 2019-01-14 |          6.8  |            134.93 | positive      |           -72.48 | 4520.65 |\n",
      "| 13 |           001 | 2019-01-15 |         15.98 |            304.73 | positive      |           125.84 | 4825.38 |\n",
      "| 14 |           001 | 2019-01-16 |          8.78 |            177.72 | positive      |           -41.68 | 5003.1  |\n",
      "| 15 |           001 | 2019-01-17 |         12.38 |            163.11 | positive      |            -8.22 | 5166.21 |\n",
      "| 16 |           001 | 2019-01-18 |          8.24 |            152.42 | positive      |            -6.55 | 5318.63 |\n",
      "| 17 |           001 | 2019-01-19 |         10.4  |            398.71 | positive      |           161.59 | 5717.34 |\n",
      "| 18 |           001 | 2019-01-20 |         21.02 |            536.83 | positive      |            34.64 | 6254.17 |\n",
      "| 19 |           001 | 2019-01-21 |         12.2  |            237.06 | positive      |           -55.84 | 6491.23 |\n",
      "| 20 |           001 | 2019-01-22 |         13.82 |            134.73 | positive      |           -43.17 | 6625.96 |\n",
      "| 21 |           001 | 2019-01-23 |         21.2  |            393.78 | positive      |           192.27 | 7019.74 |\n",
      "| 22 |           001 | 2019-01-24 |         13.1  |            235.03 | positive      |           -40.31 | 7254.77 |\n",
      "| 23 |           001 | 2019-01-25 |         13.28 |            251.14 | positive      |             6.85 | 7505.91 |\n",
      "| 24 |           001 | 2019-01-26 |          6.8  |            392.57 | positive      |            56.32 | 7898.48 |\n",
      "| 25 |           001 | 2019-01-27 |          6.8  |            264.46 | positive      |           -32.63 | 8162.94 |\n",
      "| 26 |           001 | 2019-01-28 |         10.22 |            128.43 | positive      |           -51.44 | 8291.37 |\n",
      "| 27 |           001 | 2019-01-29 |         18.14 |            384.88 | positive      |           199.68 | 8676.25 |\n",
      "| 28 |           001 | 2019-01-30 |         16.7  |            378.91 | positive      |            -1.55 | 9055.16 |\n",
      "| 29 |           001 | 2019-01-31 |         23.36 |            361.27 | positive      |            -4.66 | 9416.43 |\n",
      "| 30 |           001 | 2019-02-01 |         22.1  |            207.38 | positive      |           -42.6  | 9192.85 |\n",
      "| 31 |           001 | 2019-02-02 |         12.2  |            220.19 | positive      |             6.18 | 8996.14 |\n",
      "| 32 |           001 | 2019-02-03 |         16.88 |            503.97 | positive      |           128.88 | 9219.95 |\n",
      "| 33 |           001 | 2019-02-04 |          0    |            325.44 | positive      |           -35.42 | 9129.41 |\n",
      "| 34 |           001 | 2019-02-05 |         15.08 |            343.75 | positive      |             5.63 | 9060.96 |\n",
      "| 35 |           001 | 2019-02-06 |         16.88 |            330.9  | positive      |            -3.74 | 9226.85 |\n",
      "| 36 |           001 | 2019-02-07 |         24.26 |            330.43 | positive      |            -0.14 | 9395.76 |\n",
      "| 37 |           001 | 2019-02-08 |         20.84 |            338.07 | positive      |             2.31 | 9353.32 |\n",
      "| 38 |           001 | 2019-02-09 |          8.42 |            324.77 | positive      |            -3.93 | 9279.58 |\n",
      "| 39 |           001 | 2019-02-10 |         14.54 |            274.18 | positive      |           -15.58 | 9156.39 |\n",
      "| 40 |           001 | 2019-02-11 |         23.54 |            176.68 | positive      |           -35.56 | 8896.78 |\n",
      "| 41 |           001 | 2019-02-12 |         24.62 |            310.8  | positive      |            75.91 | 8717.27 |\n",
      "| 42 |           001 | 2019-02-13 |          9.14 |            167.89 | positive      |           -45.98 | 8750.23 |\n",
      "| 43 |           001 | 2019-02-14 |          9.32 |            153.07 | positive      |            -8.83 | 8598.57 |\n",
      "| 44 |           001 | 2019-02-15 |         12.38 |             37.53 | positive      |           -75.48 | 8458.38 |\n",
      "| 45 |           001 | 2019-02-16 |         12.56 |            196.96 | positive      |           424.81 | 8492.23 |\n",
      "| 46 |           001 | 2019-02-17 |         10.76 |            177.54 | positive      |            -9.86 | 8517.35 |\n",
      "| 47 |           001 | 2019-02-18 |         20.12 |            148.46 | positive      |           -16.38 | 8267.1  |\n",
      "| 48 |           001 | 2019-02-19 |         16.7  |            162.04 | positive      |             9.15 | 7892.31 |\n",
      "| 49 |           001 | 2019-02-20 |          8.6  |             18.27 | positive      |           -88.73 | 7673.52 |\n"
     ]
    }
   ],
   "source": [
    "# Run the query\n",
    "etl.readData(sql_queries.READ_QUERY, sql_queries.READ_QUERY_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4af807b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "etl.sqlConn.disconnectFromDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105443c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
